# E-Commerce Data Pipeline ETL

Pipeline ETL end-to-end para dados de e-commerce, integrando m√∫ltiplas fontes de dados, limpando e transformando dados brutos, e carregando-os em um banco de dados relacional para suportar an√°lises e tomada de decis√µes.

## Objective:
To build a data pipeline for an e-commerce platform that consolidates orders, customers, products, payments, and reviews into an analytical model (star schema), ready for sales, logistics, and customer satisfaction analysis.

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [M√≥dulos do Pipeline](#-m√≥dulos-do-pipeline)
- [Sistema de Configura√ß√£o](#srcutilsconfigpy---sistema-de-configura√ß√£o)
- [Sistema de Logging](#srcutilsloggerpy---sistema-de-logging)
- [Testes](#-testes)
- [Datasets](#-datasets)

## üéØ Vis√£o Geral

Este projeto implementa um pipeline ETL completo para processar dados de e-commerce da Olist, incluindo:

- **Extract (Extra√ß√£o)**: Leitura de m√∫ltiplos arquivos CSV
- **Transform (Transforma√ß√£o)**: Limpeza, valida√ß√£o e enriquecimento de dados
- **Load (Carregamento)**: Inser√ß√£o dos dados processados em banco de dados relacional

## üìÅ Estrutura do Projeto

Nota de Arquitetura
A estrutura foi pensada para equilibrar simplicidade e boas pr√°ticas.
Evitei granularidade excessiva em m√≥dulos para n√£o introduzir complexidade artificial, mantendo apenas abstra√ß√µes que seriam comuns em pipelines de produ√ß√£o de pequeno e m√©dio porte.
```
ecommerce-data-pipeline-etl/
‚îÇ
‚îú‚îÄ‚îÄ dataset/                          # Dados brutos (CSV)
‚îÇ   ‚îú‚îÄ‚îÄ olist_customers_dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ olist_geolocation_dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ olist_order_items_dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ olist_order_payments_dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ olist_order_reviews_dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ olist_orders_dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ olist_products_dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ olist_sellers_dataset.csv
‚îÇ   ‚îî‚îÄ‚îÄ product_category_name_translation.csv
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ extract.py                     # Leitura e valida√ß√£o b√°sica dos CSVs
‚îÇ   ‚îú‚îÄ‚îÄ transform.py                   # Limpeza, joins e regras de neg√≥cio
‚îÇ   ‚îú‚îÄ‚îÄ load.py                        # Escrita no banco de dados
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py                    # Orquestra√ß√£o do ETL
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ config.py                  # Leitura de configs (YAML/env)
‚îÇ       ‚îî‚îÄ‚îÄ logger.py                  # Logging centralizado
‚îÇ
‚îú‚îÄ‚îÄ notebook/
‚îÇ   ‚îú‚îÄ‚îÄ 01_exploratory_analysis.ipynb
‚îÇ   
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.yaml              # Configura√ß√µes do pipeline
‚îÇ   ‚îî‚îÄ‚îÄ dataset.yaml                # Mapeamento de datasets
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                 # Fixtures compartilhadas
‚îÇ   ‚îú‚îÄ‚îÄ test_config.py              # Testes de configura√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ test_extract.py             # Testes de extra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ test_transform.py          # Testes de transforma√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py            # Testes de pipeline
‚îÇ
‚îú‚îÄ‚îÄ logs/                           # Diret√≥rio de logs (gerado automaticamente)
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py   
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                          # Arquivos ignorados pelo Git
‚îú‚îÄ‚îÄ .env                                 # Vari√°veis de ambiente (n√£o versionado)
‚îú‚îÄ‚îÄ requirements.txt                    # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ pytest.ini                          # Configura√ß√£o do pytest
‚îú‚îÄ‚îÄ docker-compose.yml                  # Orquestra√ß√£o Docker
‚îú‚îÄ‚îÄ Dockerfile                          # Imagem Docker do ETL
‚îî‚îÄ‚îÄ README.md                           # Este arquivo
```

### Descri√ß√£o das Pastas Principais
- **`src/utils/`**: Utilit√°rios compartilhados (config, logger)
- **`config/`**: Arquivos de configura√ß√£o em formato YAML (pipeline, datasets)
- **`tests/`**: Testes unit√°rios organizados por m√≥dulo
- **`scripts/`**: Scripts de execu√ß√£o do pipeline
- **`logs/`**: Arquivos de log gerados automaticamente

## üîß M√≥dulos do Pipeline

### `src/extract.py` - M√≥dulo de Extra√ß√£o

O m√≥dulo de extra√ß√£o √© respons√°vel pela leitura e valida√ß√£o inicial dos arquivos CSV. Implementa as seguintes funcionalidades:

- **Leitura de CSVs**: Extra√ß√£o de todos os 9 datasets do diret√≥rio `dataset/raw/`
- **Valida√ß√£o de Schema**: Verifica√ß√£o autom√°tica das colunas esperadas em cada dataset
- **Tipagem Inicial**: Aplica√ß√£o de tipos de dados apropriados (string, Int64, Float64, float64) para otimiza√ß√£o de mem√≥ria e valida√ß√£o
- **Logging de Volume**: Registro detalhado de m√©tricas por dataset:
  - N√∫mero de linhas e colunas
  - Uso de mem√≥ria (MB)
  - Total de valores faltantes
  - Tempo de execu√ß√£o

**Resultados da Extra√ß√£o**: O m√≥dulo retorna um dicion√°rio contendo todos os DataFrames validados, prontos para a etapa de transforma√ß√£o. Cada dataset √© validado individualmente, garantindo que apenas dados com schema correto prossigam no pipeline. Em caso de falha na valida√ß√£o, o m√≥dulo registra erros detalhados no log e continua processando os demais datasets.

### `src/transform.py` - M√≥dulo de Transforma√ß√£o

O m√≥dulo de transforma√ß√£o √© respons√°vel pela limpeza, padroniza√ß√£o, enriquecimento e cria√ß√£o de m√©tricas derivadas. Implementa as seguintes funcionalidades:

- **Padroniza√ß√£o de Colunas**: Convers√£o para lowercase e snake_case para consist√™ncia
- **Tratamento de Valores Faltantes**: Regras espec√≠ficas por dataset (strings ‚Üí 'unknown', num√©ricos mantidos como NaN)
- **Convers√£o de Datas**: Transforma√ß√£o de strings para datetime com valida√ß√£o
- **Enriquecimento de Dados**: Merge com tradu√ß√£o de categorias de produtos, identifica√ß√£o de clientes recorrentes
- **Cria√ß√£o de M√©tricas**: 
  - Valor total do pedido (soma de itens + frete) - regra de neg√≥cio principal
  - Tempo de entrega em dias e atraso em rela√ß√£o ao estimado
  - Valida√ß√£o de coordenadas geogr√°ficas do Brasil
- **Tabela Fato Consolidada**: Cria√ß√£o de tabela fato com joins de todos os datasets e agrega√ß√µes (pagamentos, avalia√ß√µes, produtos, vendedores)

**Regras de Neg√≥cio Documentadas**: Todas as regras de transforma√ß√£o est√£o documentadas no c√≥digo, incluindo a regra principal de que o valor total do pedido √© calculado a partir da soma dos itens + frete, pois o dataset de pedidos n√£o traz esse campo consolidado.

### `src/load.py` - M√≥dulo de Carregamento

O m√≥dulo de carregamento √© respons√°vel pela inser√ß√£o dos dados no PostgreSQL, implementando uma arquitetura em duas camadas:

- **Staging Layer**: Carregamento dos dados brutos com transforma√ß√µes m√≠nimas
  - Metadados de rastreabilidade: `source` e `load_timestamp` em todas as tabelas
  - Idempot√™ncia: DELETE + INSERT por fonte, permitindo reprocessamento seguro
  - Preserva√ß√£o dos dados originais para auditoria

- **Analytics Layer (Star Schema)**: Modelo estrela otimizado para consultas anal√≠ticas
  - **Dimens√µes**: `dim_time`, `dim_customers`, `dim_products`, `dim_sellers`, `dim_geography`
  - **Tabela Fato**: `fact_orders` com m√©tricas consolidadas e foreign keys para todas as dimens√µes
  - **Idempot√™ncia**: UPSERT (ON CONFLICT) em todas as tabelas para garantir reprocessamento seguro
  - **Otimiza√ß√µes**: √çndices em foreign keys e colunas de filtro para performance

**Caracter√≠sticas**: O processo de carregamento √© totalmente automatizado, idempotente (pode ser executado m√∫ltiplas vezes sem duplicar dados) e otimizado para consultas anal√≠ticas. A configura√ß√£o √© feita via arquivos YAML e vari√°veis de ambiente.

### `src/utils/config.py` - Sistema de Configura√ß√£o

O m√≥dulo de configura√ß√£o centraliza todas as configura√ß√µes do pipeline, permitindo alterar comportamento sem modificar c√≥digo:

- **Arquivos de Configura√ß√£o**:
  - `config/pipeline.yaml`: Configura√ß√µes do pipeline (paths, database, logging, behavior)
  - `config/dataset.yaml`: Mapeamento de datasets para arquivos CSV
  - `.env`: Vari√°veis de ambiente (opcional, sobrescreve YAML)

- **Prioridade de Configura√ß√£o**: Vari√°veis de ambiente > YAML > Valores padr√£o

- **Funcionalidades**:
  - Singleton pattern para acesso global
  - Suporte a valores aninhados: `config.get('database.host')`
  - Paths como objetos `Path`: `config.data_dir`, `config.logs_dir`
  - Configura√ß√µes de banco: `config.database_config`
  - Configura√ß√µes de pipeline: `config.pipeline_config`

- **Exemplo de Uso**:
  ```python
  from src.utils.config import config
  
  # Acessar configura√ß√µes
  db_host = config.get('database.host')
  data_dir = config.data_dir
  batch_size = config.get('pipeline.batch_size')
  ```

### `src/utils/logger.py` - Sistema de Logging

O m√≥dulo de logging fornece observabilidade completa do pipeline:

- **Funcionalidades**:
  - Logging simult√¢neo em console e arquivo
  - Arquivos com timestamp: `pipeline_20240202.log`
  - N√≠veis configur√°veis (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  - Formata√ß√£o padronizada com timestamp, m√≥dulo e n√≠vel
  - Logs salvos em `logs/` (configur√°vel)

- **Integra√ß√£o**: Todos os m√≥dulos (`extract`, `transform`, `load`, `pipeline`) utilizam o logger centralizado

- **Exemplo de Uso**:
  ```python
  from src.utils.logger import get_logger
  
  logger = get_logger(__name__)
  logger.info("Processando dados...")
  logger.error("Erro ao processar", exc_info=True)
  ```

## üß™ Testes

O projeto inclui uma su√≠te de testes para garantir qualidade e confiabilidade do c√≥digo:

- **Framework**: pytest com cobertura de c√≥digo
- **Estrutura**: Testes organizados por m√≥dulo em `tests/`

### Executando Testes

```bash
# Executar todos os testes
pytest

# Com cobertura de c√≥digo
pytest --cov=src --cov-report=html

# Teste espec√≠fico
pytest tests/test_extract.py -v

# Com output detalhado
pytest -v
```

### Cobertura de Testes

- **test_config.py**: Testes do sistema de configura√ß√£o (singleton, paths, database, env vars)
- **test_extract.py**: Testes de extra√ß√£o (valida√ß√£o de schema, tipos, arquivos)
- **test_transform.py**: Testes de transforma√ß√£o (padroniza√ß√£o, tratamento de nulos, m√©tricas, tabela fato)
- **test_pipeline.py**: Testes de orquestra√ß√£o do pipeline

### Fixtures Compartilhadas

O arquivo `tests/conftest.py` fornece fixtures reutiliz√°veis:
- `sample_orders_df`: DataFrame de exemplo para pedidos
- `sample_order_items_df`: DataFrame de exemplo para itens
- `temp_data_dir`: Diret√≥rio tempor√°rio para dados de teste

## üìä Datasets

O projeto utiliza os seguintes datasets da Olist:

### `olist_customers_dataset.csv`
- **Descri√ß√£o**: Cont√©m informa√ß√µes sobre clientes e suas localiza√ß√µes.
- **Observa√ß√µes**: 
  - Cada pedido recebe um `customer_id` √∫nico.
  - O `customer_unique_id` permite identificar clientes que fizeram compras repetidas.

### `olist_geolocation_dataset.csv`
- **Descri√ß√£o**: Informa√ß√µes sobre CEPs brasileiros e suas coordenadas geogr√°ficas (latitude e longitude).

### `olist_order_items_dataset.csv`
- **Descri√ß√£o**: Informa√ß√µes sobre os itens comprados em cada pedido.

### `olist_order_payments_dataset.csv`
- **Descri√ß√£o**: Informa√ß√µes sobre as op√ß√µes de pagamento dos pedidos.

### `olist_order_reviews_dataset.csv`
- **Descri√ß√£o**: Informa√ß√µes sobre as avalia√ß√µes feitas pelos clientes.

### `olist_orders_dataset.csv`
- **Descri√ß√£o**: **Dataset principal**. A partir de cada pedido, √© poss√≠vel encontrar todas as outras informa√ß√µes relacionadas.

### `olist_products_dataset.csv`
- **Descri√ß√£o**: Informa√ß√µes sobre os produtos vendidos pela Olist.

### `olist_sellers_dataset.csv`
- **Descri√ß√£o**: Informa√ß√µes sobre os vendedores que processaram pedidos na Olist.

### `product_category_name_translation.csv`
- **Descri√ß√£o**: Tradu√ß√£o dos nomes das categorias de produtos para ingl√™s.

## üê≥ Estrutura Docker

O projeto utiliza Docker e Docker Compose para facilitar a execu√ß√£o e garantir consist√™ncia entre ambientes.

### Arquitetura Docker

O `docker-compose.yml` define dois servi√ßos:

1. **postgres**: Banco de dados PostgreSQL 15
   - Porta: `5432` (configur√°vel via `DB_PORT`)
   - Volume persistente: `postgres_data` para dados do banco
   - Healthcheck: Verifica se o banco est√° pronto antes de iniciar o ETL
   - Vari√°veis de ambiente: `DB_NAME`, `DB_USER`, `DB_PASSWORD`

2. **etl**: Container do pipeline ETL
   - Base: Python 3.11-slim
   - Depend√™ncias: Instaladas via `requirements.txt`
   - Volumes montados:
     - `./dataset` ‚Üí `/app/dataset` (dados CSV)
     - `./logs` ‚Üí `/app/logs` (arquivos de log)
   - Depend√™ncia: Aguarda o PostgreSQL estar saud√°vel antes de iniciar
   - Comando: Executa `scripts/run_pipeline.py` automaticamente

### Dockerfile

O `Dockerfile` do ETL:
- Instala depend√™ncias do sistema (gcc, postgresql-client)
- Instala depend√™ncias Python do `requirements.txt`
- Copia c√≥digo fonte, scripts e dados
- Define `PYTHONPATH` e vari√°veis de ambiente
- Executa o pipeline automaticamente ao iniciar

### Rede Docker

- Rede isolada `etl_network` conecta os servi√ßos
- O container ETL acessa o PostgreSQL pelo hostname `postgres`

## üöÄ Executando o Pipeline

### Op√ß√£o 1: Execu√ß√£o com Docker (Recomendado)

A forma mais simples de executar o pipeline completo:

```bash
# 1. Criar arquivo .env (opcional, se quiser sobrescrever defaults)
DB_HOST=localhost
DB_NAME=ecommerce_olist
DB_USER=postgres
DB_PASSWORD=postgres
DB_PORT=5432
LOAD_TO_DB=true

# 2. Executar com Docker Compose
docker compose up -d --build

# 3. Executar etl com logs
docker compose logs -f etl

# 4. Parar os containers
docker compose down

# 5. Parar e remover volumes (limpar dados do banco)
docker compose down -v
```

**O que acontece:**
1. Docker Compose inicia o PostgreSQL
2. Aguarda o banco estar pronto (healthcheck)
3. Constr√≥i a imagem do ETL
4. Inicia o container ETL que executa o pipeline automaticamente
5. Dados s√£o carregados no banco PostgreSQL

### Op√ß√£o 2: Execu√ß√£o Local (Sem Docker)

Para executar localmente, voc√™ precisa ter Python e PostgreSQL instalados:

```bash
# 1. Criar ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows

# 2. Instalar depend√™ncias
pip install -r requirements.txt

# 3. Configurar banco de dados (criar .env ou editar config/pipeline.yaml)
DB_HOST=localhost
DB_NAME=ecommerce_olist
DB_USER=postgres
DB_PASSWORD=postgres
DB_PORT=5432
LOAD_TO_DB=true

# 4. Executar pipeline
python scripts/run_pipeline.py

# Ou apenas Extract + Transform (sem carregar no banco)
# Edite config/pipeline.yaml: load_to_db: false
python scripts/run_pipeline.py
```

### Op√ß√£o 3: Executar Apenas Testes

```bash
# Instalar depend√™ncias de teste (j√° inclu√≠das no requirements.txt)
pip install -r requirements.txt

# Executar todos os testes
pytest

# Com cobertura
pytest --cov=src --cov-report=html

# Teste espec√≠fico
pytest tests/test_extract.py -v
```