# E-Commerce Data Pipeline ETL

Pipeline ETL end-to-end para dados de e-commerce, integrando mÃºltiplas fontes de dados, limpando e transformando dados brutos, e carregando-os em um banco de dados relacional para suportar anÃ¡lises e tomada de decisÃµes.

## Objective:
To build a data pipeline for an e-commerce platform that consolidates orders, customers, products, payments, and reviews into an analytical model (star schema), ready for sales, logistics, and customer satisfaction analysis.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Datasets](#datasets)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um pipeline ETL completo para processar dados de e-commerce da Olist, incluindo:

- **Extract (ExtraÃ§Ã£o)**: Leitura de mÃºltiplos arquivos CSV
- **Transform (TransformaÃ§Ã£o)**: Limpeza, validaÃ§Ã£o e enriquecimento de dados
- **Load (Carregamento)**: InserÃ§Ã£o dos dados processados em banco de dados relacional

## ğŸ“ Estrutura do Projeto

Nota de Arquitetura
A estrutura foi pensada para equilibrar simplicidade e boas prÃ¡ticas.
Evitei granularidade excessiva em mÃ³dulos para nÃ£o introduzir complexidade artificial, mantendo apenas abstraÃ§Ãµes que seriam comuns em pipelines de produÃ§Ã£o de pequeno e mÃ©dio porte.
```
ecommerce-data-pipeline-etl/
â”‚
â”œâ”€â”€ dataset/                          # Dados brutos (CSV)
â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”œâ”€â”€ olist_geolocation_dataset.csv
â”‚   â”œâ”€â”€ olist_order_items_dataset.csv
â”‚   â”œâ”€â”€ olist_order_payments_dataset.csv
â”‚   â”œâ”€â”€ olist_order_reviews_dataset.csv
â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â”œâ”€â”€ olist_products_dataset.csv
â”‚   â”œâ”€â”€ olist_sellers_dataset.csv
â”‚   â””â”€â”€ product_category_name_translation.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py                     # Leitura e validaÃ§Ã£o bÃ¡sica dos CSVs
â”‚   â”œâ”€â”€ transform.py                   # Limpeza, joins e regras de negÃ³cio
â”‚   â”œâ”€â”€ load.py                        # Escrita no banco de dados
â”‚   â”œâ”€â”€ pipeline.py                    # OrquestraÃ§Ã£o do ETL
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py                  # Leitura de configs (YAML/env)
â”‚       â””â”€â”€ logger.py                  # Logging centralizado
â”‚
â”œâ”€â”€ notebook/
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ database.yaml
â”‚   â””â”€â”€ pipeline.yaml
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â”œâ”€â”€ test_load.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.py   
â”‚
â”œâ”€â”€ .gitignore                          # Arquivos ignorados pelo Git
â”œâ”€â”€ requirements.txt                    # DependÃªncias do projeto
â”œâ”€â”€ README.md                           # Este arquivo
```

### DescriÃ§Ã£o das Pastas Principais
- **`src/utils/`**: UtilitÃ¡rios compartilhados como logging, configuraÃ§Ãµes e funÃ§Ãµes auxiliares.
- **`src/pipeline/`**: OrquestraÃ§Ã£o principal do pipeline ETL.
- **`config/`**: Arquivos de configuraÃ§Ã£o em formato YAML para facilitar manutenÃ§Ã£o.
- **`tests/`**: Testes organizados por mÃ³dulo para garantir qualidade do cÃ³digo.
- **`scripts/`**: Scripts auxiliares para setup e execuÃ§Ã£o.

## ğŸ”§ MÃ³dulos do Pipeline

### `src/extract.py` - MÃ³dulo de ExtraÃ§Ã£o

O mÃ³dulo de extraÃ§Ã£o Ã© responsÃ¡vel pela leitura e validaÃ§Ã£o inicial dos arquivos CSV. Implementa as seguintes funcionalidades:

- **Leitura de CSVs**: ExtraÃ§Ã£o de todos os 9 datasets do diretÃ³rio `dataset/raw/`
- **ValidaÃ§Ã£o de Schema**: VerificaÃ§Ã£o automÃ¡tica das colunas esperadas em cada dataset
- **Tipagem Inicial**: AplicaÃ§Ã£o de tipos de dados apropriados (string, Int64, Float64, float64) para otimizaÃ§Ã£o de memÃ³ria e validaÃ§Ã£o
- **Logging de Volume**: Registro detalhado de mÃ©tricas por dataset:
  - NÃºmero de linhas e colunas
  - Uso de memÃ³ria (MB)
  - Total de valores faltantes
  - Tempo de execuÃ§Ã£o

**Resultados da ExtraÃ§Ã£o**: O mÃ³dulo retorna um dicionÃ¡rio contendo todos os DataFrames validados, prontos para a etapa de transformaÃ§Ã£o. Cada dataset Ã© validado individualmente, garantindo que apenas dados com schema correto prossigam no pipeline. Em caso de falha na validaÃ§Ã£o, o mÃ³dulo registra erros detalhados no log e continua processando os demais datasets.

### `src/transform.py` - MÃ³dulo de TransformaÃ§Ã£o

O mÃ³dulo de transformaÃ§Ã£o Ã© responsÃ¡vel pela limpeza, padronizaÃ§Ã£o, enriquecimento e criaÃ§Ã£o de mÃ©tricas derivadas. Implementa as seguintes funcionalidades:

- **PadronizaÃ§Ã£o de Colunas**: ConversÃ£o para lowercase e snake_case para consistÃªncia
- **Tratamento de Valores Faltantes**: Regras especÃ­ficas por dataset (strings â†’ 'unknown', numÃ©ricos mantidos como NaN)
- **ConversÃ£o de Datas**: TransformaÃ§Ã£o de strings para datetime com validaÃ§Ã£o
- **Enriquecimento de Dados**: Merge com traduÃ§Ã£o de categorias de produtos, identificaÃ§Ã£o de clientes recorrentes
- **CriaÃ§Ã£o de MÃ©tricas**: 
  - Valor total do pedido (soma de itens + frete) - regra de negÃ³cio principal
  - Tempo de entrega em dias e atraso em relaÃ§Ã£o ao estimado
  - ValidaÃ§Ã£o de coordenadas geogrÃ¡ficas do Brasil
- **Tabela Fato Consolidada**: CriaÃ§Ã£o de tabela fato com joins de todos os datasets e agregaÃ§Ãµes (pagamentos, avaliaÃ§Ãµes, produtos, vendedores)

**Regras de NegÃ³cio Documentadas**: Todas as regras de transformaÃ§Ã£o estÃ£o documentadas no cÃ³digo, incluindo a regra principal de que o valor total do pedido Ã© calculado a partir da soma dos itens + frete, pois o dataset de pedidos nÃ£o traz esse campo consolidado.

### `src/load.py` - MÃ³dulo de Carregamento

O mÃ³dulo de carregamento Ã© responsÃ¡vel pela inserÃ§Ã£o dos dados no PostgreSQL, implementando uma arquitetura em duas camadas:

- **Staging Layer**: Carregamento dos dados brutos com transformaÃ§Ãµes mÃ­nimas
  - Metadados de rastreabilidade: `source` e `load_timestamp` em todas as tabelas
  - IdempotÃªncia: DELETE + INSERT por fonte, permitindo reprocessamento seguro
  - PreservaÃ§Ã£o dos dados originais para auditoria

- **Analytics Layer (Star Schema)**: Modelo estrela otimizado para consultas analÃ­ticas
  - **DimensÃµes**: `dim_time`, `dim_customers`, `dim_products`, `dim_sellers`, `dim_geography`
  - **Tabela Fato**: `fact_orders` com mÃ©tricas consolidadas e foreign keys para todas as dimensÃµes
  - **IdempotÃªncia**: UPSERT (ON CONFLICT) em todas as tabelas para garantir reprocessamento seguro
  - **OtimizaÃ§Ãµes**: Ãndices em foreign keys e colunas de filtro para performance

**CaracterÃ­sticas**: O processo de carregamento Ã© totalmente automatizado, idempotente (pode ser executado mÃºltiplas vezes sem duplicar dados) e otimizado para consultas analÃ­ticas. A configuraÃ§Ã£o Ã© feita via variÃ¡veis de ambiente (DB_HOST, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT).

## ğŸ“Š Datasets

O projeto utiliza os seguintes datasets da Olist:

### `olist_customers_dataset.csv`
- **DescriÃ§Ã£o**: ContÃ©m informaÃ§Ãµes sobre clientes e suas localizaÃ§Ãµes.
- **ObservaÃ§Ãµes**: 
  - Cada pedido recebe um `customer_id` Ãºnico.
  - O `customer_unique_id` permite identificar clientes que fizeram compras repetidas.

### `olist_geolocation_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre CEPs brasileiros e suas coordenadas geogrÃ¡ficas (latitude e longitude).

### `olist_order_items_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre os itens comprados em cada pedido.

### `olist_order_payments_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre as opÃ§Ãµes de pagamento dos pedidos.

### `olist_order_reviews_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre as avaliaÃ§Ãµes feitas pelos clientes.

### `olist_orders_dataset.csv`
- **DescriÃ§Ã£o**: **Dataset principal**. A partir de cada pedido, Ã© possÃ­vel encontrar todas as outras informaÃ§Ãµes relacionadas.

### `olist_products_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre os produtos vendidos pela Olist.

### `olist_sellers_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre os vendedores que processaram pedidos na Olist.

### `product_category_name_translation.csv`
- **DescriÃ§Ã£o**: TraduÃ§Ã£o dos nomes das categorias de produtos para inglÃªs.