# E-Commerce Data Pipeline ETL

Pipeline ETL end-to-end para dados de e-commerce, integrando mÃºltiplas fontes de dados, limpando e transformando dados brutos, e carregando-os em um banco de dados relacional para suportar anÃ¡lises e tomada de decisÃµes.

## Objective:
To build a data pipeline for an e-commerce platform that consolidates orders, customers, products, payments, and reviews into an analytical model (star schema), ready for sales, logistics, and customer satisfaction analysis.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Datasets](#datasets)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa um pipeline ETL completo para processar dados de e-commerce da Olist, incluindo:

- **Extract (ExtraÃ§Ã£o)**: Leitura de mÃºltiplos arquivos CSV
- **Transform (TransformaÃ§Ã£o)**: Limpeza, validaÃ§Ã£o e enriquecimento de dados
- **Load (Carregamento)**: InserÃ§Ã£o dos dados processados em banco de dados relacional

## ğŸ“ Estrutura do Projeto

Nota de Arquitetura
A estrutura foi pensada para equilibrar simplicidade e boas prÃ¡ticas.
Evitei granularidade excessiva em mÃ³dulos para nÃ£o introduzir complexidade artificial, mantendo apenas abstraÃ§Ãµes que seriam comuns em pipelines de produÃ§Ã£o de pequeno e mÃ©dio porte.
```
ecommerce-data-pipeline-etl/
â”‚
â”œâ”€â”€ dataset/                          # Dados brutos (CSV)
â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”œâ”€â”€ olist_geolocation_dataset.csv
â”‚   â”œâ”€â”€ olist_order_items_dataset.csv
â”‚   â”œâ”€â”€ olist_order_payments_dataset.csv
â”‚   â”œâ”€â”€ olist_order_reviews_dataset.csv
â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â”œâ”€â”€ olist_products_dataset.csv
â”‚   â”œâ”€â”€ olist_sellers_dataset.csv
â”‚   â””â”€â”€ product_category_name_translation.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py                     # Leitura e validaÃ§Ã£o bÃ¡sica dos CSVs
â”‚   â”œâ”€â”€ transform.py                   # Limpeza, joins e regras de negÃ³cio
â”‚   â”œâ”€â”€ load.py                        # Escrita no banco de dados
â”‚   â”œâ”€â”€ pipeline.py                    # OrquestraÃ§Ã£o do ETL
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py                  # Leitura de configs (YAML/env)
â”‚       â””â”€â”€ logger.py                  # Logging centralizado
â”‚
â”œâ”€â”€ notebook/
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ database.yaml
â”‚   â””â”€â”€ pipeline.yaml
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â”œâ”€â”€ test_load.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.py   
â”‚
â”œâ”€â”€ .gitignore                          # Arquivos ignorados pelo Git
â”œâ”€â”€ requirements.txt                    # DependÃªncias do projeto
â”œâ”€â”€ README.md                           # Este arquivo
```

### DescriÃ§Ã£o das Pastas Principais
- **`src/utils/`**: UtilitÃ¡rios compartilhados como logging, configuraÃ§Ãµes e funÃ§Ãµes auxiliares.
- **`src/pipeline/`**: OrquestraÃ§Ã£o principal do pipeline ETL.
- **`config/`**: Arquivos de configuraÃ§Ã£o em formato YAML para facilitar manutenÃ§Ã£o.
- **`tests/`**: Testes organizados por mÃ³dulo para garantir qualidade do cÃ³digo.
- **`scripts/`**: Scripts auxiliares para setup e execuÃ§Ã£o.

## ğŸ”§ MÃ³dulos do Pipeline

### `src/extract.py` - MÃ³dulo de ExtraÃ§Ã£o

O mÃ³dulo de extraÃ§Ã£o Ã© responsÃ¡vel pela leitura e validaÃ§Ã£o inicial dos arquivos CSV. Implementa as seguintes funcionalidades:

- **Leitura de CSVs**: ExtraÃ§Ã£o de todos os 9 datasets do diretÃ³rio `dataset/raw/`
- **ValidaÃ§Ã£o de Schema**: VerificaÃ§Ã£o automÃ¡tica das colunas esperadas em cada dataset
- **Tipagem Inicial**: AplicaÃ§Ã£o de tipos de dados apropriados (string, Int64, Float64, float64) para otimizaÃ§Ã£o de memÃ³ria e validaÃ§Ã£o
- **Logging de Volume**: Registro detalhado de mÃ©tricas por dataset:
  - NÃºmero de linhas e colunas
  - Uso de memÃ³ria (MB)
  - Total de valores faltantes
  - Tempo de execuÃ§Ã£o

**Resultados da ExtraÃ§Ã£o**: O mÃ³dulo retorna um dicionÃ¡rio contendo todos os DataFrames validados, prontos para a etapa de transformaÃ§Ã£o. Cada dataset Ã© validado individualmente, garantindo que apenas dados com schema correto prossigam no pipeline. Em caso de falha na validaÃ§Ã£o, o mÃ³dulo registra erros detalhados no log e continua processando os demais datasets.

## ğŸ“Š Datasets

O projeto utiliza os seguintes datasets da Olist:

### `olist_customers_dataset.csv`
- **DescriÃ§Ã£o**: ContÃ©m informaÃ§Ãµes sobre clientes e suas localizaÃ§Ãµes.
- **ObservaÃ§Ãµes**: 
  - Cada pedido recebe um `customer_id` Ãºnico.
  - O `customer_unique_id` permite identificar clientes que fizeram compras repetidas.

### `olist_geolocation_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre CEPs brasileiros e suas coordenadas geogrÃ¡ficas (latitude e longitude).

### `olist_order_items_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre os itens comprados em cada pedido.

### `olist_order_payments_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre as opÃ§Ãµes de pagamento dos pedidos.

### `olist_order_reviews_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre as avaliaÃ§Ãµes feitas pelos clientes.

### `olist_orders_dataset.csv`
- **DescriÃ§Ã£o**: **Dataset principal**. A partir de cada pedido, Ã© possÃ­vel encontrar todas as outras informaÃ§Ãµes relacionadas.

### `olist_products_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre os produtos vendidos pela Olist.

### `olist_sellers_dataset.csv`
- **DescriÃ§Ã£o**: InformaÃ§Ãµes sobre os vendedores que processaram pedidos na Olist.

### `product_category_name_translation.csv`
- **DescriÃ§Ã£o**: TraduÃ§Ã£o dos nomes das categorias de produtos para inglÃªs.